ğŸ“˜ Ridge and Lasso Regression
(L1 & L2 Regularization Techniques)
ğŸ” Introduction

In machine learning, regularization is a technique used to prevent overfitting by adding a penalty to the modelâ€™s complexity.

When a linear regression model becomes too sensitive to training data, it may perform poorly on unseen data. Regularization helps by constraining coefficient values.

Two popular regularization methods:

Ridge Regression (L2 Regularization)

Lasso Regression (L1 Regularization)

ğŸ§  Why Regularization is Needed

Linear regression models may suffer from:

âœ… Overfitting â€“ Model memorizes noise
âœ… Multicollinearity â€“ Highly correlated features
âœ… Large coefficients â€“ Unstable predictions

Regularization addresses these problems by penalizing large weights.

ğŸ“‰ Ridge Regression (L2 Regularization)
âœ… Definition

Ridge regression adds an L2 penalty to the loss function:

ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
=
ğ‘…
ğ‘†
ğ‘†
+
ğœ†
âˆ‘
ğ‘¤
2
Loss=RSS+Î»âˆ‘w
2

Where:

RSS = Residual Sum of Squares

Î» (lambda) = Regularization strength

wÂ² = Squared coefficients

âœ… Key Idea

ğŸ‘‰ Shrinks coefficients toward zero, but
ğŸ‘‰ Never makes them exactly zero

âœ… Characteristics

âœ” Reduces model variance
âœ” Handles multicollinearity well
âœ” Keeps all features
âœ” Improves stability

âœ… When to Use Ridge

Use Ridge when:

Many correlated predictors

All variables are potentially useful

Want coefficient shrinkage, not elimination

ğŸ“‰ Lasso Regression (L1 Regularization)
âœ… Definition

Lasso regression adds an L1 penalty:

ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
=
ğ‘…
ğ‘†
ğ‘†
+
ğœ†
âˆ‘
âˆ£
ğ‘¤
âˆ£
Loss=RSS+Î»âˆ‘âˆ£wâˆ£

Where:

|w| = Absolute value of coefficients

âœ… Key Idea

ğŸ‘‰ Shrinks coefficients
ğŸ‘‰ Can force some coefficients to exactly zero

âœ… Characteristics

âœ” Performs feature selection
âœ” Produces sparse models
âœ” Removes weak predictors
âœ” Improves interpretability

âœ… When to Use Lasso

Use Lasso when:

Dataset has many irrelevant features

Feature selection is required

Want a simpler, interpretable model

âš–ï¸ Ridge vs Lasso
Aspect	Ridge (L2)	Lasso (L1)
Penalty	Sum of squared coefficients	Sum of absolute coefficients
Coefficients	Shrunk but not zero	Some become exactly zero
Feature Selection	âŒ No	âœ… Yes
Multicollinearity	âœ… Good	âš ï¸ Can select one among correlated
Model Complexity	Medium	Simpler
ğŸ¯ Role of Lambda (Î»)

Î» controls the penalty strength:

Î» = 0 â†’ Equivalent to Linear Regression

Large Î» â†’ Strong shrinkage

Trade-off:

âœ” Higher Î» â†’ Less overfitting
âŒ Too high â†’ Underfitting

ğŸ“Š Intuition

Without Regularization:
Model may use extreme coefficient values â†’ Overfitting

With Regularization:
Coefficients constrained â†’ Better generalization

â­ Practical Benefits

âœ… Reduces overfitting
âœ… Improves generalization
âœ… Handles correlated features
âœ… Controls model complexity
âœ… Enables feature selection (Lasso)

ğŸ“ Key Takeaway

Ridge regression controls model complexity by shrinking coefficients, while Lasso regression additionally performs feature selection by driving some coefficients to zero.
