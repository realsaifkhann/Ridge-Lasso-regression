# ðŸ“˜ Ridge and Lasso Regression

### *(L1 & L2 Regularization Techniques)*

## ðŸ” Introduction

In machine learning, **regularization** is a technique used to prevent **overfitting** by adding a penalty to the modelâ€™s complexity.

When a linear regression model becomes too sensitive to training data, it may perform poorly on unseen data. Regularization helps by **constraining coefficient values**.

Two popular regularization methods:

* **Ridge Regression (L2 Regularization)**
* **Lasso Regression (L1 Regularization)**

---

# ðŸ§  Why Regularization is Needed

Linear regression models may suffer from:

âœ… **Overfitting** â€“ Model memorizes noise
âœ… **Multicollinearity** â€“ Highly correlated features
âœ… **Large coefficients** â€“ Unstable predictions

Regularization addresses these problems by penalizing large weights.

---

# ðŸ“‰ Ridge Regression (L2 Regularization)

## âœ… Definition

Ridge regression adds an **L2 penalty** to the loss function:

[
Loss = RSS + \lambda \sum w^2
]

Where:

* **RSS** = Residual Sum of Squares
* **Î» (lambda)** = Regularization strength
* **wÂ²** = Squared coefficients

---

## âœ… Key Idea

ðŸ‘‰ Shrinks coefficients **toward zero**, but
ðŸ‘‰ **Never makes them exactly zero**

---

## âœ… Characteristics

âœ” Reduces model variance
âœ” Handles multicollinearity well
âœ” Keeps all features
âœ” Improves stability

---

## âœ… When to Use Ridge

Use Ridge when:

* Many correlated predictors
* All variables are potentially useful
* Want coefficient shrinkage, not elimination

---

# ðŸ“‰ Lasso Regression (L1 Regularization)

## âœ… Definition

Lasso regression adds an **L1 penalty**:

[
Loss = RSS + \lambda \sum |w|
]

Where:

* |w| = Absolute value of coefficients

---

## âœ… Key Idea

ðŸ‘‰ Shrinks coefficients
ðŸ‘‰ Can force some coefficients to **exactly zero**

---

## âœ… Characteristics

âœ” Performs **feature selection**
âœ” Produces sparse models
âœ” Removes weak predictors
âœ” Improves interpretability

---

## âœ… When to Use Lasso

Use Lasso when:

* Dataset has many irrelevant features
* Feature selection is required
* Want a simpler, interpretable model

---

# âš–ï¸ Ridge vs Lasso

| Aspect            | Ridge (L2)                  | Lasso (L1)                         |
| ----------------- | --------------------------- | ---------------------------------- |
| Penalty           | Sum of squared coefficients | Sum of absolute coefficients       |
| Coefficients      | Shrunk but not zero         | Some become exactly zero           |
| Feature Selection | âŒ No                        | âœ… Yes                              |
| Multicollinearity | âœ… Good                      | âš ï¸ Can select one among correlated |
| Model Complexity  | Medium                      | Simpler                            |

---

# ðŸŽ¯ Role of Lambda (Î»)

Î» controls the penalty strength:

* **Î» = 0** â†’ Equivalent to Linear Regression
* **Large Î»** â†’ Strong shrinkage

Trade-off:

âœ” Higher Î» â†’ Less overfitting
âŒ Too high â†’ Underfitting

---

# ðŸ“Š Intuition

**Without Regularization:**
Model may use extreme coefficient values â†’ Overfitting

**With Regularization:**
Coefficients constrained â†’ Better generalization

---

# â­ Practical Benefits

âœ… Reduces overfitting
âœ… Improves generalization
âœ… Handles correlated features
âœ… Controls model complexity
âœ… Enables feature selection (Lasso)

---

# ðŸŽ“ Key Takeaway

> **Ridge regression controls model complexity by shrinking coefficients, while Lasso regression additionally performs feature selection by driving some coefficients to zero.**

---
